{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on #3: Evaluation and Training on Patient-specific Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will:\n",
    "1. Pre-process your personalized EMG data collected in Hands-on #2 and split it into training/validation and test sets.\n",
    "2. Load the optimized DNN from Hands-on #1 and test it (without fine-tuning) on your data.\n",
    "3. Fine-tune the DNN on your data and verify if performance improves.\n",
    "4. Lastly, quantize the model to prepare it for deployment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Personalized Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import sys\n",
    "import random\n",
    "import json\n",
    "\n",
    "from itertools import groupby\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, lfilter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torcheval.metrics import MulticlassAccuracy, Mean\n",
    "from torchinfo import summary\n",
    "\n",
    "from utils.data import EMGDataset, get_repetitions_mask, windowing\n",
    "from utils.train import CheckPoint, EarlyStopping, try_load_checkpoint\n",
    "from utils.plot import plot_raw_data, plot_signal, plot_learning_curves, plot_conf_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the paths, training configuration, training device, and reproducibility settings, as done in Hands-on #1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"./personal_data\")\n",
    "SAVE_DIR = Path(f\"experiments/hands_on_3/\")\n",
    "\n",
    "TRAINING_CONFIG = {\n",
    "    'epochs': 300,\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 0.0001,\n",
    "    'label_smoothing': 0.25,\n",
    "    'patience': 50\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Working on: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Raw Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "At the end of Hands-on #2, you should have collected your data in a binary format, and applied a simple Python script to convert it to Apache Parquet, an open-source column data format. You should have two files, called `session_1.parquet` and `session_2.parquet`. Copy both files under `./personal_data/` and load the first session to check it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the first session data and display the first few lines\n",
    "df = pd.read_parquet(DATA_DIR / \"session_1.parquet\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trigger signal was set by the GUI that you used to collect data. It assumes a value $k>0$ in correspondence of repetitions of the $k$-th gesture, and 0 during rests.\n",
    "\n",
    "Use the next utility function, provided under `./utils/plot.py` to display the first 50k samples of the raw data and trigger. The graph will be automatically colored based on the trigger to highlight gestures. With 50k samples, you should be able to see at least two types of gesture. Try to increase the to_sample parameter to visualize a longer portion of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_raw_data(df, to_sample=50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in Hands-on #1, we feed our DNNs with EMG data after an initial pre-processing phase. As a reminder, we apply the following pre-processing steps:\n",
    "\n",
    "1. We apply a 4-th order high-pass Butterworth **filter** to eliminate the continuous component from the signal. We use the scipy `butter()` function, that takes the filter order and normalized cutoff frequency as parameters, as well as the filter type (btype), and returns the corresponding coefficients. Then, `lfilter()` uses these coefficients to apply the filter to a numpy array. The normalized cutoff is computed as the absolute cutoff frequency (in Hz) divided by the Nyquist frequency (i.e. 1/2 of the sampling frequency). We apply the filtering to each EMG channel separately.\n",
    "2. We **trim** the initial and final part of each session (before the first gesture and after the last one), which contain transients. For that, we first compute the length of each segment in the trigger signal. We then remove the long first segment, corresponding to the initial acquisition before the first gesture (keeping only a small portion of it as \"rest\"). Then, we also have to eliminate all the signal after the last gesture.\n",
    "4. We apply a **min-max** normalization to have the same value ranges for all channels. We simply transform our data to x' = (x - min) / (max - min), to normalize it between 0 and 1.\n",
    "\n",
    "While the dataset used in Hands-on #1 had been pre-processed beforehand, we will have to repeat the same steps for your personal data here.\n",
    "\n",
    "Let us define some simple functions to perform these operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hp_filter(data, order, cutoff_frequency, sampling_frequency):\n",
    "    # compute the normalize cutoff frequency and apply butter() and lfilter() to filter your data\n",
    "    normalized_cutoff = cutoff_frequency / (sampling_frequency / 2)\n",
    "    b, a = butter(order, normalized_cutoff, btype=\"highpass\")\n",
    "    return lfilter(b, a, data, axis=0)\n",
    "\n",
    "def trim(data, labels):\n",
    "    segment_lengths = [sum(1 for _ in group) for _, group in groupby(labels)]\n",
    "    # eliminate most of the initial portion of the signal, where the trigger is zero, but the hand might not be yet at rest.\n",
    "    # Precisely, we keep only some seconds before the first gesture (equal to the length of the SECOND rest).\n",
    "    # Then, also eliminate all data after the last gesture, \n",
    "    # where you might have moved your hand freely. \n",
    "    start_index = segment_lengths[0] - segment_lengths[2]\n",
    "    end_index = data.shape[0] - segment_lengths[-1] + 1\n",
    "    data = data[start_index:end_index, :]\n",
    "    labels = labels[start_index:end_index]\n",
    "    return data, labels\n",
    "\n",
    "def normalize(data, min_values, max_values):\n",
    "    # normalize the data to [0:1]\n",
    "    rescaled_data = (data - min_values) / (max_values - min_values)\n",
    "    return rescaled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test these functions on the whole session_1 data and display the result. For the filtering, we use a cutoff frequency of 10Hz and a 4-th order filter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# split data and labels\n",
    "data = df.drop(\"Trigger\", axis=1)\n",
    "labels = df[\"Trigger\"].astype(int).to_numpy()\n",
    "\n",
    "# filter, trim and normalize. Call the final arrays \"normalized_data\" and \"trimmed_labels\"\n",
    "filtered_data = hp_filter(data, order=4, cutoff_frequency=10.0, sampling_frequency=500)\n",
    "trimmed_data, trimmed_labels = trim(filtered_data, labels)\n",
    "normalized_data = normalize(trimmed_data, trimmed_data.min(axis=0), trimmed_data.max(axis=0))\n",
    "\n",
    "# create a new df for plotting\n",
    "new_df = pd.DataFrame(np.hstack((normalized_data, trimmed_labels.reshape(-1,1))), columns=df.columns)\n",
    "\n",
    "plot_raw_data(new_df, to_sample=50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now see much more clearly the various channels activating in correspondance of a gesture, as well as the differences between different gestures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting and Windowing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next steps consist in separating our data into training, validation and test sets, and then constructing fixed-length, partially overlapping windows of 300 samples (as done in the pre-collected dataset) to feed our DNNs. Since this part is tedious and not particularly interesting, we provide you with two pre-cooked functions. Both are defined under `utils/data.py`. \n",
    "\n",
    "Let's start by defining the goal in terms of splitting. We want to use session_1 for **training**, and session_2 for **testing**. In fact, as you remember from hands-on #1,  generalizing over different sessions will be very important for our models. Furthermore, we also want to hold out a portion of session 1 (the _last gesture repetition_) and use it as **validation** set. Now, let's see how we can do that with the two provided functions.\n",
    "\n",
    "The first function, `get_repetitions_mask()` takes the data and labels (i.e., the trigger), as well as the index of the first/last gesture repetition of interest (two numbers between 0 and 4). It returns a binary mask of the same length as the data, containing 1s in correspondance to samples that belong to the selected repetitions (or to the associated rests) and 0s everywhere else. For example:\n",
    "```Python\n",
    "get_repetitions_mask(data, labels, 1, 3)\n",
    "```\n",
    "would return a mask with 1s in correspondence to the 2nd, 3rd and 4th repetition of each gesture.\n",
    "\n",
    "The second function, `windowing()` takes two arrays of raw data and labels (trigger), respectively of dimension $(N_{samples}, 8)$ and $N_{samples}$, and outputs the windowed data, of dimension $(N_{windows}, 300, 8)$ and the windowed labels, of dimension $N_{windows}$. Each window label is obtained looking at the trigger value in correspondence to the *center* of the window. Windows are partially overlapped, with 70% of the samples shared between consecutive windows. Furthermore, only stable windows are considered. In other words, windows that include a transition between two gestures, or are too close to it (less than 1.5s), are excluded. All parameters of this function are configurable, but we will keep the default ones.\n",
    "\n",
    "These are all the components needed to obtain our dataset, ready for DNN training and evaluation. Let's put them all together in the next function.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Task:</b> Complete the next function to return training, validation, and test data and labels after pre-processing and windowing.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(train_val_file, test_file, concatenate_all=False):\n",
    "    \n",
    "    print(f\"Processing {train_val_file}\")\n",
    "\n",
    "    # read session 1 data\n",
    "    df = pd.read_parquet(train_val_file)\n",
    "\n",
    "    # split data and labels\n",
    "    data = df.drop(\"Trigger\", axis=1).values \n",
    "    labels = df.Trigger.values.astype(int)\n",
    "\n",
    "    # apply filtering to the data \n",
    "    filtered_data = hp_filter(data, order=4, cutoff_frequency=10.0, sampling_frequency=500)\n",
    "    \n",
    "    # trim the filtered data and labels\n",
    "    trimmed_data, trimmed_labels = trim(filtered_data, labels)\n",
    "\n",
    "    # split training and validation sets using get_repetitions_mask\n",
    "    # first call the function to isolate repetitions [0,3] (for training) and\n",
    "    # [4,4] (for validation). Then, use the mask as an index in the arrays\n",
    "    # to separate the two subsets\n",
    "    train_mask = get_repetitions_mask(trimmed_data, trimmed_labels, 0, 3)\n",
    "    val_mask = get_repetitions_mask(trimmed_data, trimmed_labels, 4, 4)\n",
    "    train_data = trimmed_data[train_mask]\n",
    "    train_labels = trimmed_labels[train_mask]\n",
    "    val_data = trimmed_data[val_mask]\n",
    "    val_labels = trimmed_labels[val_mask]\n",
    "\n",
    "    print(f\"Processing {test_file}\")\n",
    "\n",
    "    # read session 2 data (test set)\n",
    "    df = pd.read_parquet(test_file)\n",
    "\n",
    "    \n",
    "    # split test data and test labels (expected: 2 lines)\n",
    "    # YOUR_CODE_START\n",
    "\n",
    "\n",
    "    # YOUR_CODE_END\n",
    "\n",
    "    # apply filtering to the test data (expected: 1 line)\n",
    "    # YOUR_CODE_START\n",
    "\n",
    "    # YOUR_CODE_END\n",
    "    \n",
    "    # trim the filtered test data and labels (expected: 1 line)\n",
    "    # YOUR_CODE_START\n",
    "\n",
    "    # YOUR_CODE_END\n",
    "\n",
    "    # we don't need to mask anything since we use the whole session 2 as test set\n",
    "    test_data, test_labels = trimmed_data, trimmed_labels\n",
    "\n",
    "    # normalize all data arrays using the TRAINING SET's min and max values\n",
    "    train_min, train_max = train_data.min(axis=0), train_data.max(axis=0)\n",
    "    train_data = normalize(train_data, train_min, train_max)\n",
    "    val_data = normalize(val_data, train_min, train_max)\n",
    "    test_data = normalize(test_data, train_min, train_max)\n",
    "\n",
    "    # create windows for all three datasets\n",
    "    train_data, train_labels = windowing(train_data, train_labels)\n",
    "    val_data, val_labels = windowing(val_data, val_labels)\n",
    "    test_data, test_labels = windowing(test_data, test_labels)\n",
    "\n",
    "    # note: the concatenate_all option allows you to obtain splits in which:\n",
    "    # - train = train + test\n",
    "    # - val = val\n",
    "    # - test = train + test\n",
    "    # this is needed if you want to retrain on ALL DATA before deployment (which is usually a good idea).\n",
    "    # of course, in this case, the test accuracy shouldn't be considered as relevant.\n",
    "    # for now, leave it at False, then you might come back and set it to True before deployment.\n",
    "    if concatenate_all:\n",
    "        train_data = test_data = np.vstack((train_data, test_data))\n",
    "        train_labels = test_labels = np.hstack((train_labels, test_labels))\n",
    "    \n",
    "    return (train_data, train_labels), (val_data, val_labels), (test_data, test_labels), (train_min, train_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the function also returns the training set minimum and maximum values. We will need those values if we want to normalize newly incoming data. So, at the end of this notebook, we will save them to file. \n",
    "\n",
    "Invoke the function just defined to generate our three data splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train, val, test, norm_values = prepare_data(DATA_DIR / \"session_1.parquet\", DATA_DIR / \"session_2.parquet\")\n",
    "train_data, train_labels = train\n",
    "val_data, val_labels = val\n",
    "test_data, test_labels = test\n",
    "norm_min, norm_max = norm_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the size of our datasets. Note that the test set size is quite large compared to training and validation. One could be tempted to add more data to the training set. However, remember that it is fundamental to test DNN models for EMG gesture recognition on *different sessions* compared to those use for training, to make sure that they are practically useful (still predicting reasonably even with a slightly different sensor mount).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print dataset shapes \n",
    "print(f\"Total Train Windows: {train_data.shape[0]}, Val Windows: {val_data.shape[0]}, Test Windows: {test_data.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reuse the plot_signal function seen in Hands-on #1 to visualize one window of the pre-processed training data and see if they looks similar to those treated in that session. Change the two window indexes (5 and 30 by default) to find rest and gesture windows respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_signal(5, 30, train_data, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move on, let's save the preprocessed and splitted data to file, so that we can re-use it later (e.g. to run some test inferences on the target hardware):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(SAVE_DIR / \"preprocessed_data\", exist_ok=True)\n",
    "np.save(SAVE_DIR / \"preprocessed_data\" / \"train_data\", train_data)\n",
    "np.save(SAVE_DIR / \"preprocessed_data\" / \"train_labels\", train_labels)\n",
    "np.save(SAVE_DIR / \"preprocessed_data\" / \"val_data\", val_data)\n",
    "np.save(SAVE_DIR / \"preprocessed_data\" / \"val_labels\", val_labels)\n",
    "np.save(SAVE_DIR / \"preprocessed_data\" / \"test_data\", test_data)\n",
    "np.save(SAVE_DIR / \"preprocessed_data\" / \"test_labels\", test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Together with the data, we also ought to save the minimum and maximum values for normalization. In fact, differently from other preprocessing steps (windowing and filtering) normalization has data dependent parameters (the min/max values themselves). At inference time, we will have to reuse the **same normalization constants** to obtain good predictions from the DNN. The next cell generates a simple JSON file containing the two values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{SAVE_DIR}/rescaling_values.json\", \"w\") as f:\n",
    "    json.dump({\"min\": norm_min.tolist(), \"max\": norm_max.tolist()}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets and Dataloaders\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last data preparation step consists in generating Dataset instances (first) and DataLoaders (second) for PyTorch trainings. For the first part, we can reuse the `EMGDataset` class seen in Hands-on #1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the datasets for training, validation and test sets\n",
    "train_ds = EMGDataset(train_data, train_labels)\n",
    "val_ds = EMGDataset(val_data, val_labels)\n",
    "test_ds = EMGDataset(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concerning DataLoaders, we can reuse the function prepared in Hands-on #1 to build them. Using the `ipynb` library, we can conveniently load function definitions from another notebook. Let's use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.defs.hands_on_1 import build_dataloaders\n",
    "\n",
    "train_dl, val_dl, test_dl = build_dataloaders(TRAINING_CONFIG, train_ds, val_ds, test_ds, num_workers=4)\n",
    "print(f\"Training data-loader length: {len(train_dl)}\")\n",
    "print(f\"Validation data-loader length: {len(val_dl)}\")\n",
    "print(f\"Test data-loader length: {len(test_dl)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have much fewer minibatches, as expected since we're working with a single patient's data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Zero-shot Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Hands-on #1, we claimed that EMG processing requires subject-specific training. To verify this, let's try to load our optimized DNN from that session, which is fine-tuned on our multi-patient dataset, and directly test it on the newly created test set from your personal data.\n",
    "\n",
    "Load the model saved at the end of Hands-on #1, which should be under `./experiments/hands_on_1/final_model.pt`. If you did not manage to finish that session, you can find a pre-cooked model under `./checkpoints/hands_on_1/final_model.pt`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = Path(\"./experiments/hands_on_1/final_model.pt\")\n",
    "model = torch.load(MODEL_PATH).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now evaluate this model on the new data. As for `build_dataloaders` let's not rewrite any code. Use the `ipynb` library to import the `evaluate()` function from Hands-on #1, and run it. Remember that evaluate also requires the loss function. It's not strictly necessary here, but to avoid having to rewrite anything, we will just create a new `criterion` instance before testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete this line to import the right functions\n",
    "from ipynb.fs.defs.hands_on_1 import evaluate, get_criterion\n",
    "\n",
    "# define the criterion, then evaluate the loaded model on the new test set and print the result. \n",
    "criterion = get_criterion(train_dl, TRAINING_CONFIG, device)\n",
    "test_loss, test_acc, test_macro_acc = evaluate(model, criterion, test_dl, device, num_classes=9)\n",
    "print(f\"Test Loss: {test_loss:.2f}, Test Acc: {test_acc:.2f}, Test Macro Acc: {test_macro_acc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our model performs quite poorly on the new data. You should get a macro accuracy not higher than 30-40% at most. This confirms our hypothesis on the need of personalized training. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Question:</b> Why do you think the accuracy is so low? What influences the EMG signals?\n",
    "</div>\n",
    "\n",
    "So, the next obvious step is to fine-tune the model on the new data. Let's do that next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Patient-specific Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model on the new data. Again, no need to rewrite anything here. Just import the training loop defined in Hands-on #1 and use it. Note from the `TRAINING_CONFIG` at the beginning of this notebook, that we're training for more epochs, with a longer \"patience\", and with a lower learning rate. This is possible because we have less data. Let's see if it improves the final results.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Task:</b> Complete the next cell to train the loaded model on the new data.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the training loop function from hands-on-1 \n",
    "from ipynb.fs.defs.hands_on_1 import training_loop\n",
    "\n",
    "# run the training loop, saving checkpoints in SAVE_DIR/fine_tuning (expected: 1 line)\n",
    "# YOUR_CODE_START\n",
    "history = training_loop(SAVE_DIR / 'fine_tuning', TRAINING_CONFIG, ...)\n",
    "# YOUR_CODE_END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you know very well how to analyze the output of the training. Use the next cells (you can add more if you need to) to verify how the training went (plotting the learning curves). Then evaluate the model on the test set and plot the confusion matrix. All needed code has been introduced in Hands-on #1.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Task:</b> Complete the next cells as you want, to check the results of your model (plot learning curves,  confusion matrices, etc.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use this cell to plot learning curves, evaluate the model on the test set, and visualize the confusion matrix\n",
    "# (expected: 4-5 lines)\n",
    "# YOUR_CODE_START\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# YOUR_CODE_END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Questions:</b> \n",
    "\n",
    "- Did the model recover the lost accuracy? \n",
    "- Is there still a gap between validation and test set? Why?\n",
    "- Looking at the confusion matrix, are the mistakes made by the model \"reasonable\"?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the (Scripted) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move on with quantization, let's save the fine-tuned floating point model to disk. This will be useful for testing real-time inference on your laptop.\n",
    "This time, we will use **torchscript** format for saving. This is a format that allows us to easily reload the model in a different script, even with a different environment and directory structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_scripted = torch.jit.script(model)\n",
    "model_scripted.save(SAVE_DIR / 'final_model_scripted.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All DNN models considered up to now used **32-bit floating point** for internal operations, and for storing weights and activations. However, our hardware target only supports Quantized DNN inference, using **8-bit integers**. Therefore, we need to convert our model to that format before we can export it and compile it.\n",
    "\n",
    "Simply quantizing a model by replacing all floating point data with their closest integer approximation (the most basic form of Post-Training Quantization) could worsen its accuracy. Fortunately, this drop can often be recovered by running some epochs of the so-called **Quantization-Aware Training (QAT)**. Essentially, QAT is a training that \"simulates\" the fact that weights and activations will be quantized, and allows the gradient-descent-based optimizer to modify the weights in order to compensate the approximation error introduced by quantization.\n",
    "\n",
    "The **[PLiNIO](https://github.com/eml-eda/plinio) DNN optimization library**, introduced in Hands-on #1, can be used to perform QAT on our model, as well as allowing to export the final \"full integer\" model in a format compatible with the compiler used in one of the next sessions.  \n",
    "\n",
    "More precisely, PLiNIO's QAT function is embedded in the `MPS()` class, which is used to perform a more advanced optimization, called **Mixed-Precision Search**. \n",
    "We will not use MPS in this session, since our target hardware and backend library do not support $<8$ bit inference (*). However, we can still use PLiNIO to perform a simple QAT run, by simply reducing it to a **\"corner case\" of MPS, with a single precision** (8-bit) to select from.\n",
    "\n",
    "If you're interested in the details on the MPS algorithm present in PLiNIO, check-out these two papers: [link1](https://arxiv.org/abs/2206.08852), [link2](https://arxiv.org/abs/2004.05795). After you finish this hands-on, feel free to also try applying MPS with multiple precisions on our DNN for EMG gesture recognition, as an extra. Although we won't be able to deploy models with precisions different from 8-bit, it could still be interesting to check how much we can compress the weights without losing too much accuracy.\n",
    "\n",
    " \n",
    "(*) Actually, the DNN accelerator present in GAP9 would support those precisions, but we will only deploy on the multi-core RISC-V cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing for QAT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing the required PLiNIO classes and functions. Some of these will be needed only for the final export:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plinio.methods import MPS\n",
    "from plinio.methods.mps import get_default_qinfo\n",
    "from plinio.methods.mps.quant.quantizers import PACTAct\n",
    "from plinio.methods.mps.quant.backends import Backend, integerize_arch\n",
    "from plinio.methods.mps.quant.backends.match import MATCHExporter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have to apply the `MPS()` constructor to our patient-specific fine-tuned DNN. The constructor expects the following inputs:\n",
    "\n",
    "- The model to be converted\n",
    "- The cost metric to be optimized (this is for an actual MPS run, since we have a single precision, we can ignore this parameter)\n",
    "- The shape of a single input (for internal graph conversion passes)\n",
    "- A `qinfo` dictionary, containing settings on the desired type of Quantization to apply for different parts of the network.\n",
    "\n",
    "The settings in `qinfo` include the quantization algorithm to use for weights and activations (e.g. min-max, PaCT, etc), and optional configuration parameters. We do not have time to go over these details, but please refer to survey papers such as [this one](https://arxiv.org/abs/2106.08295) for more information. In our case, it suffices to use the reasonable default settings provided by PLiNIO, by calling the `get_default_qinfo()` function. This function expects as input parameters the tuple of weights and activations bitwidths to be included in the optimization (in our case, only 8-bit for both).\n",
    "\n",
    "There's just one thing to customize in the default `qinfo`, namely the range of the DNN **input** quantizer. In fact, since we know that our (float) data is in the $[0, 1]$ range, we can set the initial range of the quantizer to be the same. This should facilitate the conversion.\n",
    "\n",
    "Let's create our `MPS()` instance.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Task:</b> Complete the next cell to create an instance of the MPS() class with the correct parameters.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default qinfo dictionary, specifying 8-bit as the only precision for both weights and activations\n",
    "qinfo = get_default_qinfo((8,), (8,))\n",
    "\n",
    "# modify the default qinfo for the input layer, since we're using signed data in the [0, 1] range\n",
    "qinfo['input_default']['quantizer'] = PACTAct\n",
    "qinfo['input_default']['kwargs'] = {'init_clip_val': +1}\n",
    "\n",
    "model_copy = copy.deepcopy(model)\n",
    "# create a MPS() instance passing model_copy and the correct parameters to the constructor\n",
    "# don't forget to move the newly created model to our training device (expected: 2-3 lines)\n",
    "# YOUR_CODE_START\n",
    "\n",
    "\n",
    "# YOUR_CODE_END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model generated by the MPS constructor has approximated weights and operations that simulate int8 precision. Furthermore, other optimizations are performed during the conversion, such as folding Batch Normalization layers with Convolutions or Linear layers, to avoid entirely their execution in the final deployed model. Overall, the result of the conversion is similar to what we would get with a (very basic) post-training quantization. Let's check how this model performs on our test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the post-conversion mps_model on the test set and print the results\n",
    "test_loss, test_acc, test_macro_acc = evaluate(mps_model, criterion, test_dl, device, num_classes=9)\n",
    "print(f\"Test Loss: {test_loss:.2f}, Test Acc: {test_acc:.2f}, Test Macro Acc: {test_macro_acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interesting!!**\n",
    "\n",
    "You should see that the test accuracy is preserved quite well, possibly with a minimal drop! It seems that, for our simple task, Post-Training Quantization could suffice.\n",
    "We could stop here, and directly export the model for deployment. But since QAT is very useful on more complex problems, let's see how we could run it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running QAT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual execution of QAT is nothing more than an additional training run, using the model generated by the `MPS()` constructor. Note that, if we wanted to actually *select* the bitwidth using MPS, we would have to run something similar to the `nas_loop` seen in Hands-on #1. However, we're keeping a fixed precision, and do not aim to optimize the DNN cost (e.g. total memory occupation) in this phase. Or better, we already reduced memory by a factor of 4 by moving from float32 to int8, but now, our goal is just retrieving the lost accuracy.\n",
    "\n",
    "So, in our case, a simple `training_loop` on the converted model will suffice.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Task:</b> Complete the next cell to run a standard training loop on the MPS model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run the training loop, saving checkpoints in SAVE_DIR/qat (expected: 1 line)\n",
    "# YOUR_CODE_START\n",
    "history = training_loop(SAVE_DIR / 'qat', TRAINING_CONFIG, ....)\n",
    "# YOUR_CODE_END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the final model after QAT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the post-conversion mps_model on the test set and print the results\n",
    "test_loss, test_acc, test_macro_acc = evaluate(mps_model, criterion, test_dl, device, num_classes=9)\n",
    "print(f\"Test Loss: {test_loss:.2f}, Test Acc: {test_acc:.2f}, Test Macro Acc: {test_macro_acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After QAT, it is possible that your model has become even **slightly more accurate** than the floating point version! This sometimes happens when quantizing: the approximation introduced by quantization has a *regularizing* effect, which makes the model behave slightly better on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Question:</b> Combining Channel-pruning with PIT (in Hands-on #1) and 8-bit Quantization (in this notebook), by how much did we compress the model in total? Where do you think there's room to reduce it even more?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export for Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now call the `.export()` method of the PLiNIO MPS model, exactly as done in Hands-on #1 for PIT. Notice however, that in this case, the exporting phase has a slightly different behaviour. In fact, rather than outputting a model that includes standard torch layers, we replace each quantized layer with a new class (for instance, `nn.Conv2D` becomes `QuantConv2D`). These layers function analogously to the torch equivalents, but also store the quantization parameters (e.g. min/max values for each weight tensor), and use them to simulate the effect of quantization during inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_mps_model = copy.deepcopy(mps_model)\n",
    "quant_model = final_mps_model.export()\n",
    "quant_model = quant_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, the model exported by MPS still only applies a so-called \"fake quantization\". This means that the DNN is not yet using *only* integer data. Rather, some parameters, such as the scale factors for (re-)quantization are still in floating point. To deploy on our target, however, **all data should be integer**. For instance, in the PULP-NN backend library that we will use on our target, the multiplication times a floating point scaling factor is replaced by the sequence of: i) an integer multiplication and ii) a right shift.\n",
    "\n",
    "To obtain a model that fully complies with this execution model, we need a further conversion step. This is implemented by the next cell, which calls the `integerize_arch` function with parameters that specify the desired backend, among the supported ones, and some other optional parameters. The backend essentially refers to the compiler that will be used to take the model and convert it to inference code for the hardware. In our case, it will be the [MATCH](https://github.com/eml-eda/match) compiler.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# convert the model to full-integer, compiler-compliant format\n",
    "full_int_model = integerize_arch(quant_model, Backend.MATCH, backend_kwargs={'shift_pos': 16})\n",
    "full_int_model = full_int_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the previous conversion removes all floating point operations from the network, it might affect the accuracy (minimally). So, let's verify by how much.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate the full-integer model on the test set and print the results\n",
    "test_loss, test_acc, test_macro_acc = evaluate(full_int_model, criterion, test_dl, device, num_classes=9)\n",
    "print(f\"Test Loss: {test_loss:.2f}, Test Acc: {test_acc:.2f}, Test Macro Acc: {test_macro_acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have to save the final model. In this case, differently from Hands-on #1, we don't need to save it *just* in PyTorch format. Rather, we want to generate an ONNX file compatible with what the MATCH compiler expects. The following cell does that. \n",
    "\n",
    "It essentially generates the ONNX using torch's built-in utility, and then adds some custom annotations as desired by MATCH. Let's run it (you can safely ignore the warnings that appear).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# export to onnx (after making a copy of the model to avoid destroying it).\n",
    "single_batch_shape = (1,) + train_ds[0][0].shape\n",
    "exporter = MATCHExporter()\n",
    "exporter.export(copy.deepcopy(full_int_model), single_batch_shape, SAVE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least, we also have to save a new rescaling parameter for later reuse. In fact, while the floating point model takes as input normalized samples in the range $[0, 1]$, the full-integer model also uses **8-bit input data**. \n",
    "\n",
    "The MPS class in PLiNIO (and the following integerization step) use an \"Input Quantizer\" layer to perform this transformation. The Input Quantizer is of type `PACTAct`, as specified in the `qinfo` data structure above. This type of quantizer implements the following transformation:\n",
    "\n",
    "$$X_{int} = \\left\\lfloor \\frac{255}{\\alpha} \\cdot \\min(X_{float}, \\alpha) \\right\\rfloor $$\n",
    "\n",
    "\n",
    "where $\\alpha$ is a **learned** clipping value.\n",
    "\n",
    "Note that, in order to run the full pipeline on GAP9, we would also need to run the **filtering** using integers. However, for simplicity, in the following sessions we will still perform filtering *in floating point*, then rescale the (float) data to $[0, 1]$, and finally convert them to integers.  Overall, the complete normalization pipeline will be the following:\n",
    "\n",
    "$$X\\mathrm{\\ (Unnormalized,\\ filtered\\ input\\ window)} \\rightarrow X_{float} = \\frac{X - \\min(X)}{\\max(X) - \\min(X)} \\rightarrow X_{int} =  \\left\\lfloor \\frac{255}{\\alpha} \\cdot \\min(X_{float}, \\alpha) \\right\\rfloor  $$\n",
    "\n",
    "\n",
    "Let's extract the clipping value from the full integer model, and save it on disk for later usage.\n",
    "\n",
    "We will update the same JSON file used for the floating point scaling normalization factors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtz = full_int_model.input_1_input_quantizer\n",
    "clip_val = float(qtz.out_quantizer.clip_val)\n",
    "print(f\"Input clipping value: {clip_val}\")\n",
    "\n",
    "with open(f\"{SAVE_DIR}/rescaling_values.json\", \"r\") as f:\n",
    "    scaling_dict = json.load(f)\n",
    "\n",
    "scaling_dict['clip_val'] = clip_val\n",
    "with open(f\"{SAVE_DIR}/rescaling_values.json\", \"w\") as f:\n",
    "    json.dump(scaling_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will probably see that the clipping value remained very close to the initialization value (+1). That is, our model prefers to avoid  any clipping, and map the entire input range to the int8 range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retraining on all data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: Do this only if you have spare time, your model will work regardless.\n",
    "\n",
    "\n",
    "One important thing that you might want to do before the next Hands-on session is relaunching this script (after changing `SAVE_DIR`), setting `concatenate_all=True` in the `prepare_data()` function. This will retrain everything using the combined training and test sets as training set (and keeping just a small held-out validation set for things like early-stopping).\n",
    "\n",
    "This can give you a slightly more accurate final model, since it will be trained on data from both sessions. Of course, the test accuracies computed in this notebook won't be meaningful anymore, but doing this is fair, since your actual \"test\" data will be the one that your model will encounter in-field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other things you could play with include:\n",
    "- Using PLiNIO to actually perform a mixed-precision search on the model, and see if you can compress it even more without losing accuracy. Remember: this won't be supported by our hardware target, but remains an interesting test. It will require you to use a `nas_loop` similar to the one seen in Hands-on #1, and impose a cost metric such as `params_bit`, which estimates the total model size in bits (thus accounting for bitwidth), whereas `params` only *counts* the parameters.\n",
    "- Playing with the training configuration (learning rate, batch size, patience, type of optimizer, label smoothing, etc) to try and further increase the test accuracy of the model.\n",
    "- etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "efcl_venv",
   "language": "python",
   "name": "efcl_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
